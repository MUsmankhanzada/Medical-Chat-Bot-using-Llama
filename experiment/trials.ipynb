{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c29ea992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import ctransformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b185241f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Medical-Chat-Bot-using-Llama\\env_chatbot\\lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone.embeddings import PineconeEmbeddings\n",
    "from langchain_pinecone.vectorstores import PineconeVectorStore\n",
    "from pinecone import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33f51d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7bc5ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                             glob = \"*.pdf\",\n",
    "                             loader_cls = PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3d638af",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text = load_pdf(\"D:\\Medical-Chat-Bot-using-Llama\\data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fba52eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracted_text\n",
    "# print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c7e014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def download_huggingface_embedings():\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fd48ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_13208\\1412518668.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "embeddings = download_huggingface_embedings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d8d2f2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ef000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = embeddings.embed_query(\"Hello, world! How are you?\")\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f405d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_splitter(extracted_text):\n",
    "    text_split = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)\n",
    "    return text_split.split_documents(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2664178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = text_splitter(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "314e986a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4251"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9861b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_chunks\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "# print(PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0c5e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(\"medical-bot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58a2633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1d444e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "uuids = [str(uuid4()) for _ in range(len(text_chunks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e453019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add_documents(documents=text_chunks, ids=uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "23ba9eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Downloaded to: ./models\\models--jlallas--Meta-Llama-3-8B-Instruct-Q4_K_M-GGUF\\snapshots\\e05e01f6dfd1aff3c39778d049aa351b233c7439\\meta-llama-3-8b-instruct-q4_k_m.gguf\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"jlallas/Meta-Llama-3-8B-Instruct-Q4_K_M-GGUF\",\n",
    "    filename=\"meta-llama-3-8b-instruct-q4_k_m.gguf\",\n",
    "    cache_dir=\"./models\"\n",
    ")\n",
    "print(\"‚úÖ Downloaded to:\", model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2e39ab18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available? False\n",
      "Number of GPUs: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available?\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7fe963ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=512,\n",
    "    n_threads=8,\n",
    "    temperature=0.3,        # Default creativity level\n",
    "    top_p=0.9,              # Nucleus sampling threshold\n",
    "    frequency_penalty=0.0,  # Reduce repeated words\n",
    "    presence_penalty=0.0,   # Penalize repeating existing tokens\n",
    "    repeat_penalty=1.1,     # Discourage excessive looping\n",
    "    n_predict=128, \n",
    "    use_mmap=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "eeb30cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è Response:  Hypertension, or high blood pressure, is a condition in which the blood pressure in the arteries is elevated above normal levels. The primary symptoms of hypertension may vary depending on the individual and the severity of the condition.\n"
     ]
    }
   ],
   "source": [
    "response = llm.create_completion(\n",
    "    prompt=\"You are a medical assistant. What are the primary symptoms of hypertension?\",\n",
    "    max_tokens=100,\n",
    "    stop=[\"\\n\"]\n",
    ")\n",
    "\n",
    "print(\"üó£Ô∏è Response:\", response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ad5869b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'page': 127.0, 'source': 'D:\\\\Medical-Chat-Bot-using-Llama\\\\data\\\\Medical_book.pdf'}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 2114\\nAllergies\\nGEM - 0001 to 0432 - A  10/22/03 1:42 PM  Page 114'), Document(metadata={'page': 135.0, 'source': 'D:\\\\Medical-Chat-Bot-using-Llama\\\\data\\\\Medical_book.pdf'}, page_content='foreign organisms. This reaction between antibody and\\nantigen sets off a series of reactions designed to protect\\nthe body from infection. Sometimes, this same series of\\nreactions is triggered by harmless, everyday substances.\\nThis is the condition known as allergy, and the offend-\\ning substance is called an allergen. Common inhaled\\nallergens include pollen, dust, and insect parts from tiny\\nhouse mites. Common food allergens include nuts, fish,\\nand milk.\\nAllergic reactions involve a special set of cells in\\nthe immune system known as mast cells. Mast cells\\nserve as guards in the tissues where the body meets the\\noutside world: the skin, the mucous membranes of the')]\n"
     ]
    }
   ],
   "source": [
    "query = \"Whate are allergies\"\n",
    "results = vector_store.similarity_search(query, k=2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3378e4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e75f6945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "langchain_llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    n_ctx=512,\n",
    "    n_threads=8,\n",
    "    temperature=0.3,\n",
    "    top_p=0.9,\n",
    "    repeat_penalty=1.1,\n",
    "    n_gpu_layers=0,  # Set >0 if GPU acceleration\n",
    "    use_mmap=True,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "642a3d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "078ec23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "chain_type_kwargs = {\"prompt\": prompt}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2f8e3347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=langchain_llm,  # ‚úÖ use the wrapped version\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(\n",
    "        search_type=\"similarity_score_threshold\",\n",
    "        search_kwargs={\"k\": 2, \"score_threshold\": 0.4}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d8afe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acne is a skin condition that occurs when pores or hair follicles become blocked. This allows a waxy material, sebum, to collect inside the pores or follicles. Normally, sebum flows out onto the skin and hair to form a protective coating, but when it cannot get out, small swellings develop on the skin surface. Bacteria and dead skin cells can also collect that can cause inflammation and lead to acne. 224\n",
      "GEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 24\n",
      "Acne is a skin condition that occurs when pores or hair follicles become blocked. This allows a waxy material, sebum, to collect inside the pores or follicles. Normally, sebum flows out onto the skin and hair to form a protective coating, but when it cannot get out, small swellings develop on the skin surface. Bacteria and dead skin cells can also collect that can cause inflammation and lead to acne. 224\n",
      "GEM - 0001 to 0432 - A  10/22/03 1:41 PM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acne is not curable, although long-term control is achieved in up to 60% of patients treated with conventional therapies. Wholistic physicians or nutritionists can recommend the proper amounts of herbs such as cnidium seed (Cnidium monnieri) and honeysuckle flower (Lonicera japonica). \n",
      "GALE ENCYCLOPEDIA OF MEDICINE 226\n",
      "Acne\n",
      "GEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 26\n",
      "GALE ENCYCLOPEDIA OF MEDICINE 224\n",
      "Acne\n",
      "GEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 24\n",
      "Question: how can we cure acne \n",
      "Answer: Acne is not curable, although long-term control is achieved in up to 60% of patients treated with conventional therapies. Wholistic physicians or nutritionists can recommend the proper amounts of herbs such as cnidium seed (Cnidium monnieri) and honeysuckle flower (Lonicera japonica). \n",
      "GALE ENCYCLOPEDIA OF MEDICINE 226\n",
      "Acne\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"Enter your question: \")\n",
    "    result = qa.invoke({\"query\": user_input})\n",
    "    print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "824ef1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\".\\models\\models--jlallas--Meta-Llama-3-8B-Instruct-Q4_K_M-GGUF\\snapshots\\e05e01f6dfd1aff3c39778d049aa351b233c7439\\meta-llama-3-8b-instruct-q4_k_m.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d43ff1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models\\tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Medical-Chat-Bot-using-Llama\\env_chatbot\\lib\\site-packages\\huggingface_hub\\commands\\download.py:139: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.\n",
      "  warnings.warn(\n",
      "Downloading 'tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf' to 'models\\.cache\\huggingface\\download\\R1oO1JDp1WeHaU9noAw_5AdMnsM=.9fecc3b3cd76bba89d504f29b616eedf7da85b96540e490ca5824d3f7d2776a0.incomplete'\n",
      "Download complete. Moving file to models\\tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli download TheBloke/TinyLLaMA-1.1B-Chat-v1.0-GGUF tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf --local-dir ./models --local-dir-use-symlinks False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f13ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\".\\models\\tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "450ff53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Retrieving relevant documents...\n",
      "‚úÖ Retrieval took: 0.15s\n",
      "‚úÖ LLM response took: 26.66s\n",
      "\n",
      "üí° Answer:\n",
      "- Black widow spider (Venomous)\n",
      "- Brown recluse spider (Venomous)\n",
      "- Scorpions (Venomous)\n",
      "- Tick bites (Lethal)\n",
      "- Ant bites (Lethal)\n",
      "- Chiroptera (Bats)\n",
      "- Cercopithecidae (Monkeys, apes, and relatives)\n",
      "- Canis lupus familiaris (Dogs)\n",
      "- Felidae (Cats)\n",
      "- Rattus norvegicus (Rats)\n",
      "- Hyaenidae (Hyenas)\n",
      "- Viverridae (Tigers)\n",
      "- Felis silvestris (Lions)\n",
      "- Felis catus (Domestic Cats)\n",
      "- Felis domesticus (\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "import time\n",
    "\n",
    "# ‚úÖ Configure LlamaCpp (optimized settings)\n",
    "langchain_llm = LlamaCpp(\n",
    "    model_path=model_path,  # Path to quantized GGUF model (e.g., llama-2-7b-chat.Q4_K_M.gguf)\n",
    "    n_ctx=512,              # Reduced context window for faster inference\n",
    "    n_threads=8,            # Utilize multiple CPU threads\n",
    "    temperature=0.3,        # Keep generation deterministic\n",
    "    top_p=0.9,\n",
    "    repeat_penalty=1.1,\n",
    "    use_mmap=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# ‚úÖ Prompt Template\n",
    "prompt_template = \"\"\"\n",
    "Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# ‚úÖ Retrieval chain with faster top_k search\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"k\": 2}  # Top 2 similar documents\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": prompt}\n",
    "\n",
    "# ‚úÖ Build RetrievalQA chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=langchain_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs\n",
    ")\n",
    "\n",
    "# ‚úÖ Context caching\n",
    "cached_context = None\n",
    "\n",
    "# Interactive loop\n",
    "while True:\n",
    "    user_input = input(\"Enter your question: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # ‚úÖ Measure Pinecone retrieval time\n",
    "    if cached_context is None:\n",
    "        start_retrieve = time.time()\n",
    "        print(\"üîç Retrieving relevant documents...\")\n",
    "        retrieved_docs = retriever.get_relevant_documents(user_input)\n",
    "        print(f\"‚úÖ Retrieval took: {time.time() - start_retrieve:.2f}s\")\n",
    "        cached_context = retrieved_docs\n",
    "    else:\n",
    "        print(\"‚ôªÔ∏è Using cached context...\")\n",
    "\n",
    "    # ‚úÖ Measure LLM response time\n",
    "    start_llm = time.time()\n",
    "    result = qa.invoke({\"query\": user_input})\n",
    "    print(f\"‚úÖ LLM response took: {time.time() - start_llm:.2f}s\")\n",
    "\n",
    "    # Display result\n",
    "    print(\"\\nüí° Answer:\")\n",
    "    print(result[\"result\"])\n",
    "\n",
    "    # Ask if user wants to keep the same context\n",
    "    reuse = input(\"\\nüîÑ Use same context for next question? (y/n): \")\n",
    "    if reuse.lower() != \"y\":\n",
    "        cached_context = None  # Reset cache\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
